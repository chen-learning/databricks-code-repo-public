{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2fa543d-6d58-48f9-8f4d-077f95574f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Day68 Lakehouse Operations\n",
    "- Earlier what we learned is Datalake Operations\n",
    "  - reading and writing data in storage\n",
    "  - Where data stored in the form of file(s) inside Volumes\n",
    "\n",
    "- Lakehouse build on top of Datalake\n",
    "- Data stored in the form of tables in Lakehouse; nothing but creating Data warehouses\n",
    "- DataLAKE + Data wareHOUSE => LAKEHOUSE\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ff8c8c-e6d5-4c49-9a67-6775b832aef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dataframe \n",
    "ingest_df1 = spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header\",header=True,inferSchema=True)\n",
    "display(ingest_df1.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5cd6264-7b88-4cc8-a0f8-7a0450b6c07d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## We are building delta table in databricks\n",
    "  - Hive table in onperm (storage HDFS)\n",
    "  - BigQuery Table in GCP (storage GFS)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7644b4-03bf-4499-9ee2-fcccbee3ddd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_df1.write.mode(\"overwrite\").saveAsTable(\"workspace.wd36schema.lh_custtbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5746309-e90c-4e1a-9bf4-8904465863ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"show create table  workspace.wd36schema.lh_custtbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4897102-10ff-4067-a54f-d23eabeef479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read from Lakehouse\n",
    "#### Use spark engine to pull the data from the lakehouse backed by DBFS (AWS S3)  (nothing but dalalake) where data in delta format(deltalake)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581aceeb-faf3-449d-b767-681b324c7347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark engine + lakehouse + datalake(deltalake)\n",
    "spark.sql(\"select * from workspace.wd36schema.lh_custtbl\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a2ebd0-01f2-4f35-8308-0d61ce82008c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cludera/Horton Works|Databricks|Laptop\n",
    "--------------------|----------|------\n",
    "Cluster|workspace(catalog)|MyComputer\n",
    "**Database/Schema (Metastore)**|**Database/Schema**|**HardDisk**\n",
    "Table(SQL)|  Tables(SQL)|Tables(SQL)\n",
    ". | Volumes(DBFS)| Volumes(FS)\n",
    "**FileSystem (HDFS)**|No Seperate File System|No Seperate File System\n",
    "Volumes(FS)|.|.\n",
    "\n",
    "\n",
    "HDF - Hadoop Distributer File System\n",
    "DBFS - DataBricks File System\n",
    "FS - File System\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709e1283-a9ed-42c5-a163-7b94deeb80ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image_1769357809053.png](./image_1769357809053.png \"image_1769357809053.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aaa94aa-1a00-4724-b5ca-37069994bf75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "delta table\n",
    "| cid | fname | lname | age | prof     |\n",
    "|-----|-------|-------|-----|----------|\n",
    "| 1   | first | last  | 33  | engineer |\n",
    "\n",
    "file to insert/load\n",
    "cid,prof, age, fname, lname\n",
    "2,doctor,33,fisrtname,lastname\n",
    "\n",
    "**Postional Notation** -> data order/position should be followed\n",
    "| cid | fname | lname | age | prof     |\n",
    "|-----|-------|-------|-----|----------|\n",
    "| 1   | first | last  | 33  | engineer |\n",
    "| 2   |doctor | 44    |fisrtname|lastname|  \n",
    "\n",
    "**Named Notation** -> column name should match\n",
    "| cid | fname | lname | age | prof     |\n",
    "|-----|-------|-------|-----|----------|\n",
    "| 1   | first | last  | 33  | engineer |\n",
    "| 2   |fisrtname|lastname|44|doctor|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02f6490-5d60-4d86-bfd8-0f26256642db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# insertInto - Postional Notation of storing data\n",
    "# for sequenthil load (few records; NOT for bulk load) ; its slow\n",
    "# it will not create new table; insert records into exisitng table; we can safely use only if table exists\n",
    "\n",
    "ingest_df1.write.insertInto(\"workspace.wd36schema.lh_custtbl\",overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b37850eb-b5db-41e3-8d8b-7749847bcc4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## advance -simple intro\n",
    "#### performanc improvements\n",
    "- jdbc, partition by, bucket by, cluster by sort by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41083a9e-1c49-4e27-90cf-186ad5c030fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_df1.write.partitionBy(\"age\").csv(\"/Volumes/workspace/wd36schema/ingestion_volume/target/custs_age_partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69534182-0572-449e-9277-f0b049ac5903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Try various read options \n",
    "- formats like csv, json, xml, serialized (orc/parquest/delta), table(delta/hive)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day68_Lakehouse_Operations_Intro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
