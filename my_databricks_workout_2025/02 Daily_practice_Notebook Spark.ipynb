{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc672207-34c9-4823-b775-3ddcd09e81df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Day 63,64 (2025-12-10,11)\n",
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716c3660-cfb1-48f4-9e7c-8a264e7bda6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOT requuired in databricks notebook since spark session is already created and available\n",
    "from pyspark.sql import SparkSession\n",
    "spark1 = SparkSession.builder.appName(\"Car Demo\").getOrCreate()\n",
    "print(id(spark) , \" - ID of spark created and given to us by Databricks\")\n",
    "print(id(spark1), \" - ID of spark created by us; its not created but get from existing instance by SparkSession.builder.getOrCreate() method\")\n",
    "print(\"Both IDs will be same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e0708eb-8274-4b0f-95b5-377f4bc3be56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Programatically create schema, volume and folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cd909b4-d1b3-42bb-9d27-767b1397dbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Directory structure / Hierarchy \n",
    "### catalog/workspace -> Schema/database -> volume -> directory/folder -> data/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74143515-238e-4191-9c47-b4199aca7a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Catalog / Workspace\n",
    "create catalog if not exists catalog1_dropme;\n",
    "\n",
    "-- Schema / Database\n",
    "-- create schema if not exists catalog1_dropme.schema1;\n",
    "-- OR\n",
    "create database if not exists catalog1_dropme.schema1;\n",
    "\n",
    "--volume\n",
    "create volume if not exists catalog1_dropme.schema1.volume1;\n",
    "\n",
    "-- folders/directory will be created using file system %fs  / dbutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa83708-f7a8-4b30-9b66-02c318a6a87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/catalog1_dropme/schema1/volume1/source\")\n",
    "dbutils.fs.put(\"/Volumes/catalog1_dropme/schema1/volume1/source/test.txt\",\"something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f9b6ca-8bcf-4c8c-b520-44f6d961dc82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists wd36schema;\n",
    "create volume if not exists wd36schema.ingestion_volume;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768e1d6f-c93a-4293-a8d6-368174de4943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/workspace/wd36schema/ingestion_volume/source\")\n",
    "dbutils.fs.put(\"/Volumes/workspace/wd36schema/ingestion_volume/source/test.txt\",\"something\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25e3ce5e-778b-41e3-ab4a-8a9f7213c88c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read data from csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "633d9bbe-1f85-4082-b863-a8a0f291e99d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CSV - using spark.read.csv -> \"Dataframe\"\n",
    "(path: PathOrPaths)\n",
    "\n",
    "<br><br>\n",
    "(path: PathOrPaths, schema: Optional[Union[StructType, str]]=None, sep: Optional[str]=None, encoding: Optional[str]=None, quote: Optional[str]=None, escape: Optional[str]=None, comment: Optional[str]=None, header: Optional[Union[bool, str]]=None, inferSchema: Optional[Union[bool, str]]=None, ignoreLeadingWhiteSpace: Optional[Union[bool, str]]=None, ignoreTrailingWhiteSpace: Optional[Union[bool, str]]=None, nullValue: Optional[str]=None, nanValue: Optional[str]=None, positiveInf: Optional[str]=None, negativeInf: Optional[str]=None, dateFormat: Optional[str]=None, timestampFormat: Optional[str]=None, maxColumns: Optional[Union[int, str]]=None, maxCharsPerColumn: Optional[Union[int, str]]=None, maxMalformedLogPerPartition: Optional[Union[int, str]]=None, mode: Optional[str]=None, columnNameOfCorruptRecord: Optional[str]=None, multiLine: Optional[Union[bool, str]]=None, charToEscapeQuoteEscaping: Optional[str]=None, samplingRatio: Optional[Union[float, str]]=None, enforceSchema: Optional[Union[bool, str]]=None, emptyValue: Optional[str]=None, locale: Optional[str]=None, lineSep: Optional[str]=None, pathGlobFilter: Optional[Union[bool, str]]=None, recursiveFileLookup: Optional[Union[bool, str]]=None, modifiedBefore: Optional[Union[bool, str]]=None, modifiedAfter: Optional[Union[bool, str]]=None, unescapedQuoteHandling: Optional[str]=None) -> \"DataFrame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae077d0f-e351-446f-a587-dc48ffc862d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CSV - using spark.read.csv -> \"Dataframe\"\n",
    "/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1 -> comma(',') seperted with no header\n",
    "/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header -> comma(',') seperted with header\n",
    "/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header_oth_del -> other delimiter ('~') with header\n",
    "\n",
    "<br>\n",
    "#### 1. path: PathOrPaths; No other options\n",
    "#### By default, \n",
    "##### separator / delimiter is ','\n",
    "##### header as _c0, _c1..._cn; n-> no of columns\n",
    "##### all columns aas string type \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d511c6-45e9-4808-9942-f3f050f0a5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# data with comma seperated and NO column header\n",
    "csv_df1 =spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\")\n",
    "print(csv_df1.printSchema())\n",
    "display(csv_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649fb05b-b5f2-4034-a344-b08dcc18df24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data with comma seperated and column header\n",
    "csv_df1a =spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header\")\n",
    "print(csv_df1a.printSchema())\n",
    "display(csv_df1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b57b453-79bd-4766-8987-5b5b65a82099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### other delimiter\n",
    "#### If seperator not provided, everting will be consider as single column (if no comma present in the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8293ca68-d3cb-43b0-8f8f-2f2fbf95a088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "csv_df2=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header_oth_del\")\n",
    "print(csv_df2.printSchema())\n",
    "display(csv_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "439dfeab-5c37-44cb-a311-befc4995499b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### with seperator option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce9873c7-17e6-48f4-a439-d9ebbb25c776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df2a=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header_oth_del\",sep='~')\n",
    "print(csv_df2a.printSchema())\n",
    "display(csv_df2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ba84e9c-b6f5-4661-9066-563f141cb406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Header concepts\n",
    "#### Define our own column names using toDF fnction by providing teh header columns\n",
    "##### instead of default _c0,_c1... define our own column names\n",
    "#### Use column names in the data; enabling header=True by considering first row as header columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69867a7c-a627-4b5b-81c8-0a2335a3ff4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data with comma seperated and NO column header\n",
    "csv_df1 =spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\").toDF(\"custid\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n",
    "#display(csv_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7e4e5c-e143-447c-8f99-076650ea85d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data with comma seperated and column header\n",
    "csv_df1a =spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header\", header=True )\n",
    "print(csv_df1a.printSchema())\n",
    "csv_df1a.show(2)\n",
    "#display(csv_df1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e53de3e-e988-40d9-a24f-6ebe61996ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Infer Schema\n",
    "### **Performance Consideration** need more causious to use this function\n",
    "### It scans entire data if sampling not given and **imediately evaluating and executing** \n",
    "#### NOT good for large data \n",
    "#### NOT good to use in predefined schema datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987781cf-1284-4c94-b597-e46928f5520d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 =spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\").toDF(\"custid\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.where(\"custid in (4004979,4004981)\").show()\n",
    "csv_df1 =spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\",inferSchema=True).toDF(\"custid\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.where(\"custid in (4004979,4004981)\").show()\n",
    "print(\"******************************************\")\n",
    "csv_df1 =spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_2\").toDF(\"custid\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.where(\"custid in (4004979,4004981)\").show()\n",
    "csv_df1 =spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_2\",inferSchema=True).toDF(\"custid\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.where(\"custid in (4004979,4004981)\").show()\n",
    "#display(csv_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "970286a8-12c7-4566-8a50-415ac78a30a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delimiter / Seperator options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28daf3b-b9ea-4637-935e-2ccd522dad03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df2=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header_oth_del\").toDF(\"my_col_name\")\n",
    "print(csv_df2.printSchema())\n",
    "csv_df2.show(2)\n",
    "\n",
    "csv_df2.createOrReplaceTempView(\"csv_df2\")\n",
    "spark.sql(\"desc csv_df2\").show()\n",
    "\n",
    "spark.sql(\"select my_col_name from csv_df2\").show()\n",
    "\n",
    "spark.sql(\"select split(my_col_name,'~') as splitted from csv_df2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208a34cb-d67f-4585-9166-02004fa1a5a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df2=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header_oth_del\", header=True)\n",
    "print(csv_df2.printSchema())\n",
    "csv_df2.show(2)\n",
    "\n",
    "csv_df2.createOrReplaceTempView(\"csv_df2\")\n",
    "spark.sql(\"desc csv_df2\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"select * from csv_df2\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"select `custid~fname~lname~age~profession` from csv_df2\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"select split(`custid~fname~lname~age~profession`,'~') as splitted from csv_df2\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"select split(`custid~fname~lname~age~profession`,'~') as splitted from csv_df2\").createOrReplaceTempView(\"csv_df2a\")\n",
    "spark.sql(\"select splitted[0],splitted[1],splitted[2],splitted[3],splitted[4] from csv_df2a\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5d4366-93f5-499f-8fe6-06d1b982f109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df2a=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header_oth_del\", header=True, sep='~')\n",
    "print(csv_df2a.printSchema())\n",
    "csv_df2a.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a21435-24f8-4f4c-8d03-d64d93bf3c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generic read option other than usal formats(csv,json,xml,..) like bigquery, redshift,athena, synapse...tmp folder, access control.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fd80b6-0330-4209-8be7-fc5dd4526fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df3=spark.read.options(header=True, inferSchema=True, sep='~').format('csv').load(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header_oth_del\")\n",
    "print(csv_df3.printSchema())\n",
    "csv_df3.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01adfd69-fdfb-4255-9217-4c1015134f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read from multiple files\n",
    "#### Read multiple files from a path (using regular expression/pathGlobFilter)\n",
    "#### Read multiple files from multiple paths (using list of paths)\n",
    "#### Read multiple files from path(s) and sub directories (using recurviseFileLookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4524033f-2243-42d6-9beb-a842565b90b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_df1 = spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\", header=True, inferSchema=True, sep='~')\n",
    "print(\"1\", csv_df1.count())\n",
    "csv_df2 = spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_2\", header=True, inferSchema=True, sep='~')\n",
    "print(\"2\", csv_df2.count())\n",
    "csv_df3 = spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header\", header=True, inferSchema=True, sep='~')\n",
    "print(\"3\", csv_df3.count())\n",
    "csv_df4 = spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header_oth_del\", header=True, inferSchema=True, sep='~')\n",
    "print(\"4\", csv_df4.count())\n",
    "\n",
    "csv_df5 = spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs*\", header=True, inferSchema=True, sep='~')\n",
    "print(\"5\", csv_df5.count())\n",
    "\n",
    "csv_df6 = spark.read.csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\",\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_2\"], header=True, inferSchema=True, sep='~')\n",
    "print(\"6\", csv_df6.count())\n",
    "\n",
    "csv_df7 = spark.read.csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/NY/\",\"/Volumes/workspace/wd36schema/ingestion_volume/source/TX/\"], header=True, inferSchema=True, sep=',', pathGlobFilter=\"custs_header_*\", recursiveFileLookup=True)\n",
    "print(\"7\", csv_df7.count())  \n",
    "csv_df7.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "662c43bd-1cd5-4e46-8adf-ef67bb6fd31b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Custom Schema - Define the schema structure\n",
    "### Using String Structure\n",
    "### Using StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45c2094a-befa-413a-98a5-b6293bddf842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Using String Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1763640-4814-48e7-8eb0-2f30f9d83bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 = spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\", header=True, inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n",
    "\n",
    "\n",
    "str_struct = \"id integer,fname string,lname string,age integer,profession string\"\n",
    "\n",
    "csv_df2 = spark.read.schema(str_struct).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\", header=True)\n",
    "print(csv_df2.printSchema())\n",
    "csv_df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa702c36-3fd4-4be0-a719-709d106c2b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Using  StructType and DataType class\n",
    "##### Steps to define the cutom schema usinfg StructTyre\n",
    "1. Import StructType and DataType libraries/classes<br>\n",
    "2. Intanstiate StructType object<br>\n",
    "3. Create StructFiled object for each column which contains column name, data type and nullable True/False<br>\n",
    "4. Pass as list argument to teh StructType object\n",
    "e.g **from pyspark.sql.type import StructType,StructFiled,IngeterType,StringType...** <br>\n",
    "define_structure = StructType([StructField('column_name1, DataType(), False, StructField('column_name2, DataType(), True)),...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2a549ad-db34-4143-b914-d7f43052dbdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "custom_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False), \n",
    "    StructField(\"fname\", StringType(), True), \n",
    "    StructField(\"lname\", StringType(), True), \n",
    "    StructField(\"age\", IntegerType(), True), \n",
    "    StructField(\"profession\", StringType(), True)\n",
    "])\n",
    "\n",
    "csv_df7 = spark.read.schema(custom_schema).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\", header=True,enforceSchema=True)\n",
    "print(csv_df7.printSchema())\n",
    "csv_df7.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "008138e4-bc78-4b55-87f9-15d8c971af85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6615440794098659,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02 Daily_practice_Notebook Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
